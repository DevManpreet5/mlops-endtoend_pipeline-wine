{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(r'/Users/s/Desktop/mlops/mlops_wine_end2end_pipeline/artifacts/data/transformed/train.csv')\n",
    "test_df=pd.read_csv(r'/Users/s/Desktop/mlops/mlops_wine_end2end_pipeline/artifacts/data/transformed/test.csv')\n",
    "\n",
    "y_train = train_df['Class'] \n",
    "X_train = train_df.drop('Class', axis=1)\n",
    "\n",
    "y_test = test_df['Class'] \n",
    "X_test = test_df.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:11<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 513\n",
      "[LightGBM] [Info] Number of data points in the train set: 142, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 1.964789\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n",
      "Model                                                                         \n",
      "AdaBoostRegressor                            1.00       1.00  0.00        0.09\n",
      "ExtraTreesRegressor                          0.94       0.96  0.15        0.06\n",
      "SVR                                          0.93       0.95  0.16        0.01\n",
      "NuSVR                                        0.93       0.95  0.17        0.02\n",
      "KNeighborsRegressor                          0.92       0.95  0.18        0.01\n",
      "BaggingRegressor                             0.90       0.94  0.19        0.02\n",
      "HistGradientBoostingRegressor                0.86       0.91  0.23        0.28\n",
      "LGBMRegressor                                0.85       0.90  0.24        0.12\n",
      "RandomForestRegressor                        0.82       0.89  0.25        0.18\n",
      "HuberRegressor                               0.81       0.88  0.26        0.01\n",
      "LassoLarsCV                                  0.81       0.88  0.26        0.03\n",
      "LarsCV                                       0.81       0.88  0.26        0.03\n",
      "LassoCV                                      0.81       0.88  0.26        0.07\n",
      "ElasticNetCV                                 0.81       0.88  0.26        0.04\n",
      "LassoLarsIC                                  0.81       0.88  0.26        0.02\n",
      "TransformedTargetRegressor                   0.81       0.88  0.26        0.01\n",
      "LinearRegression                             0.81       0.88  0.26        0.01\n",
      "Lars                                         0.81       0.88  0.26        0.02\n",
      "Ridge                                        0.81       0.88  0.26        0.01\n",
      "BayesianRidge                                0.81       0.88  0.26        0.01\n",
      "RidgeCV                                      0.81       0.88  0.27        0.01\n",
      "SGDRegressor                                 0.81       0.88  0.27        0.01\n",
      "RANSACRegressor                              0.81       0.88  0.27        0.04\n",
      "LinearSVR                                    0.81       0.88  0.27        0.01\n",
      "OrthogonalMatchingPursuitCV                  0.78       0.86  0.28        0.01\n",
      "PoissonRegressor                             0.77       0.85  0.29        0.05\n",
      "MLPRegressor                                 0.74       0.84  0.31        0.26\n",
      "GammaRegressor                               0.73       0.83  0.32        0.01\n",
      "TweedieRegressor                             0.71       0.82  0.32        0.01\n",
      "PassiveAggressiveRegressor                   0.65       0.78  0.36        0.02\n",
      "ExtraTreeRegressor                           0.62       0.76  0.37        0.01\n",
      "GradientBoostingRegressor                    0.61       0.75  0.38        0.11\n",
      "OrthogonalMatchingPursuit                    0.60       0.75  0.38        0.01\n",
      "DecisionTreeRegressor                        0.55       0.71  0.41        0.01\n",
      "XGBRegressor                                 0.49       0.68  0.43        0.15\n",
      "ElasticNet                                  -0.27       0.20  0.68        0.01\n",
      "DummyRegressor                              -0.64      -0.03  0.77        0.01\n",
      "LassoLars                                   -0.64      -0.03  0.77        0.01\n",
      "Lasso                                       -0.64      -0.03  0.77        0.01\n",
      "QuantileRegressor                           -0.67      -0.05  0.78        8.68\n",
      "GaussianProcessRegressor                    -2.46      -1.17  1.13        0.02\n",
      "KernelRidge                                 -9.62      -5.67  1.97        0.28\n"
     ]
    }
   ],
   "source": [
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.0s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.0s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   0.8s\n",
      "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   0.8s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   0.5s\n",
      "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.0s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.4s\n",
      "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   0.0s\n",
      "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   0.7s\n",
      "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.3s\n",
      "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.1s\n",
      "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.7s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END ......learning_rate=1, loss=linear, n_estimators=50; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.2s\n",
      "[CV] END ......learning_rate=1, loss=linear, n_estimators=50; total time=   0.2s\n",
      "[CV] END ......learning_rate=1, loss=linear, n_estimators=50; total time=   0.0s\n",
      "[CV] END ......learning_rate=1, loss=linear, n_estimators=50; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.3s\n",
      "[CV] END ......learning_rate=1, loss=linear, n_estimators=50; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.3s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=100; total time=   0.2s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=100; total time=   0.3s\n",
      "[CV] END ......learning_rate=1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END ......learning_rate=1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=200; total time=   0.3s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=100; total time=   0.1s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=100; total time=   0.3s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=100; total time=   0.2s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=200; total time=   0.1s\n",
      "[CV] END ......learning_rate=1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END ......learning_rate=1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END .learning_rate=1, loss=exponential, n_estimators=50; total time=   0.1s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=200; total time=   0.6s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=100; total time=   0.3s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=100; total time=   0.3s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=200; total time=   0.0s\n",
      "[CV] END .....learning_rate=1, loss=linear, n_estimators=200; total time=   0.5s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=100; total time=   0.2s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=100; total time=   0.0s\n",
      "[CV] END .learning_rate=1, loss=exponential, n_estimators=50; total time=   0.1s\n",
      "[CV] END .learning_rate=1, loss=exponential, n_estimators=50; total time=   0.1s\n",
      "[CV] END ......learning_rate=1, loss=square, n_estimators=50; total time=   0.1s\n",
      "[CV] END .learning_rate=1, loss=exponential, n_estimators=50; total time=   0.1s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=100; total time=   0.1s\n",
      "[CV] END .learning_rate=1, loss=exponential, n_estimators=50; total time=   0.1s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=200; total time=   0.4s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=100; total time=   0.1s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=200; total time=   0.4s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=100; total time=   0.2s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=200; total time=   0.1s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=100; total time=   0.2s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=200; total time=   0.5s\n",
      "[CV] END learning_rate=1, loss=exponential, n_estimators=200; total time=   0.3s\n",
      "[CV] END .....learning_rate=1, loss=square, n_estimators=200; total time=   0.4s\n",
      "Best Parameters found:  {'learning_rate': 1, 'loss': 'square', 'n_estimators': 200}\n",
      "Best Cross-Validation Score (MSE):  -0.019668497002645068\n",
      "R2 Score:  0.9523809523809523\n",
      "Mean Absolute Error:  0.027777777777777776\n",
      "Mean Squared Error:  0.027777777777777776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ada_boost_model = AdaBoostRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],             \n",
    "    'learning_rate': [0.01, 0.1, 1],             \n",
    "    'loss': ['linear', 'square', 'exponential']   \n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=ada_boost_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameters found: \", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score (MSE): \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"R2 Score: \", r2_score(y_test, y_pred))\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
